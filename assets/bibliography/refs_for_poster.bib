@article{cacheteux_king_2022_MOUS,
author = {Caucheteux, Charlotte and King, Jean-Rémi},
year = {2022},
month = {02},
pages = {134},
title = {Brains and algorithms partially converge in natural language processing},
volume = {5},
journal = {Communications Biology},
doi = {10.1038/s42003-022-03036-1}
}

@inproceedings{muller-eberstein-etal-2022-probing,
    title = "Probing for Labeled Dependency Trees",
    author = {M{\"u}ller-Eberstein, Max  and
      van der Goot, Rob  and
      Plank, Barbara},
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.532",
    doi = "10.18653/v1/2022.acl-long.532",
    pages = "7711--7726",
    abstract = "Probing has become an important tool for analyzing representations in Natural Language Processing (NLP). For graphical NLP tasks such as dependency parsing, linear probes are currently limited to extracting undirected or unlabeled parse trees which do not capture the full task. This work introduces DepProbe, a linear probe which can extract labeled and directed dependency parse trees from embeddings while using fewer parameters and compute than prior methods. Leveraging its full task coverage and lightweight parametrization, we investigate its predictive power for selecting the best transfer language for training a full biaffine attention parser. Across 13 languages, our proposed method identifies the best source treebank 94{\%} of the time, outperforming competitive baselines and prior work. Finally, we analyze the informativeness of task-specific subspaces in contextual embeddings as well as which benefits a full parser{'}s non-linear parametrization provides.",
}

@article{DBLP:journals/corr/Dozatmanning_biaffine_parser16,
  author       = {Timothy Dozat and
                  Christopher D. Manning},
  title        = {Deep Biaffine Attention for Neural Dependency Parsing},
  journal      = {CoRR},
  volume       = {abs/1611.01734},
  year         = {2016},
  doi          = {10.48550/arXiv.1611.01734}
}

@article{janmathijs_schoffelen_2019_mother_of_unificatio,
  author = {{J.M.} Schoffelen and Robert Oostenveld and Nietzsche Lam and Julia Uddén and Annika Hultén and Peter Hagoort},
  title = {{Mother of Unification Studies, a 204-subject multimodal neuroimaging dataset to study language processing}},
  year = 2019,
  publisher = {Radboud University},
  version = 1,
  doi = {10.34973/37n0-yc51}
}
@inproceedings{Abnar2017ExperientialDA,
  title={Experiential, Distributional and Dependency-based Word Embeddings have Complementary Roles in Decoding Brain Activity},
  author={Samira Abnar and Rasyan Ahmed and Max Mijnheer and Willem H. Zuidema},
  booktitle={Workshop on Cognitive Modeling and Computational Linguistics},
  doi={10.18653/v1/W18-0107},
  year={2017}
}
@inproceedings{Beek2001TheAD,
  title={The Alpino Dependency Treebank},
  author={Leonoor van der Beek and Gosse Bouma and Robert Malouf and Gertjan van Noord},
  booktitle={Computational Linguistics in the Netherlands 2001},
  doi={10.1163/9789004334038_003},
  year={2001}
}
@article{XLM-v,
author = {Liang, Davis and Gonen, Hila and Mao, Yuning and Hou, Raymond and Goyal, Naman and Ghazvininejad, Marjan and Zettlemoyer, Luke and Khabsa, Madian},
year = {2023},
title = {XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models},
doi = {10.48550/arXiv.2301.10472}
}
@inproceedings{delobelle-etal-2020-robbert,
    title = "{R}ob{BERT}: a {D}utch {R}o{BERT}a-based {L}anguage {M}odel",
    author = "Delobelle, Pieter  and
      Winters, Thomas  and
      Berendt, Bettina",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    year = 2020,
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2020.findings-emnlp.292",
    pages = "3255--3265"
}
@inproceedings{jawahar-etal-2019-bert,
    title = "What Does {BERT} Learn about the Structure of Language?",
    author = "Jawahar, Ganesh  and
      Sagot, Beno{\^\i}t  and
      Seddah, Djam{\'e}",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1356",
    doi = "10.18653/v1/P19-1356",
    pages = "3651--3657",
    abstract = "BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT{'}s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures.",
}
@article{devries2019bertje,
      title={BERTje: A Dutch BERT Model},
      author={Wietse de Vries and Andreas van Cranenburgh and Arianna Bisazza and Tommaso Caselli and Gertjan van Noord and Malvina Nissim},
      year={2019},
      doi={10.48550/arXiv.1912.09582},
      primaryClass={cs.CL}
}
@article{Udden_2022,
    author = {Uddén, Julia and Hultén, Annika and Schoffelen, Jan-Mathijs and Lam, Nietzsche and Harbusch, Karin and van den Bosch, Antal and Kempen, Gerard and Petersson, Karl Magnus and Hagoort, Peter},
    title = "{Supramodal Sentence Processing in the Human Brain: fMRI Evidence for the Influence of Syntactic Complexity in More Than 200 Participants}",
    journal = {Neurobiology of Language},
    volume = {3},
    number = {4},
    pages = {575-598},
    year = {2022},
    month = {09},
    abstract = "{This study investigated two questions. One is: To what degree is sentence processing beyond single words independent of the input modality (speech vs. reading)? The second question is: Which parts of the network recruited by both modalities is sensitive to syntactic complexity? These questions were investigated by having more than 200 participants read or listen to well-formed sentences or series of unconnected words. A largely left-hemisphere frontotemporoparietal network was found to be supramodal in nature, i.e., independent of input modality. In addition, the left inferior frontal gyrus (LIFG) and the left posterior middle temporal gyrus (LpMTG) were most clearly associated with left-branching complexity. The left anterior temporal lobe showed the greatest sensitivity to sentences that differed in right-branching complexity. Moreover, activity in LIFG and LpMTG increased from sentence onset to end, in parallel with an increase of the left-branching complexity. While LIFG, bilateral anterior temporal lobe, posterior MTG, and left inferior parietal lobe all contribute to the supramodal unification processes, the results suggest that these regions differ in their respective contributions to syntactic complexity related processing. The consequences of these findings for neurobiological models of language processing are discussed.}",
    issn = {2641-4368},
    doi = {10.1162/nol_a_00076},
    url = {https://doi.org/10.1162/nol\_a\_00076},
    eprint = {https://direct.mit.edu/nol/article-pdf/3/4/575/2046817/nol\_a\_00076.pdf},
}
