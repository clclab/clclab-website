- title: Brain Reader 
  date: 2019-03-27
  authors: Samira Abnar, Rasyan Ahmed, Max Mijnheer, Willem Zuidema
  reference: CMCLâ€™18
  link: https://lang.science.uva.nl/neuro_semantics/
  description: >-
    We evaluate 8 different word embedding models on their usefulness 
    for predicting the neural activation patterns associated with 
    concrete nouns. The models we consider include an experiential model, 
    based on crowd-sourced association data, several popular neural and 
    distributional models, and a model that reflects the syntactic context 
    of words (based on dependency parses). Our goal is to assess the 
    cognitive plausibility of these various embedding models, and 
    understand how we can further improve our methods for interpreting 
    brain imaging data.

    We show that neural word embedding models exhibit superior performance 
    on the tasks we consider, beating experiential word representation model. 
    The syntactically informed model gives the overall best performance 
    when predicting brain activation patterns from word embeddings; the 
    GloVe distributional method gives the overall best performance when 
    predicting in the reverse direction (words vectors from brain images). 
    Interestingly, however, the error patterns of these different models are 
    markedly different. This may support the idea that the brain uses 
    different systems for processing different kinds of words. Moreover, 
    we suggest that taking the relative strengths of different embedding 
    models into account will lead to better models of the brain activity 
    associated with words.

- tite: "Processing Arithmetics: Diagnostic Classifier Demo"
  date: 2019-03-27
  authors: Dieuwke Hupkes, Sara Veldhoen, Willem Zuidema
  link: https://lang.science.uva.nl/arithmetic/
  reference: >- 
    Dieuwke Hupkes, Sara Veldhoen, Willem Zuidema.
    Visualisation and 'diagnostic classifiers' reveal how recurrent 
    and recursive neural networks process hierarchical structure

  paper_link: https://arxiv.org/abs/1711.10203
  description: >-
    We investigate how neural networks can learn and process languages 
    with hierarchical, compositional semantics. To this end, we define 
    the artificial task of processing nested arithmetic expressions, and 
    study whether different types of neural networks can learn to compute 
    their meaning. We find that recursive neural networks can find a 
    generalising solution to this problem, and we visualise this solution 
    by breaking it up in three steps: project, sum and squash. As a next 
    step, we investigate recurrent neural networks, and show that a gated 
    recurrent unit, that processes its input incrementally, also performs 
    very well on this task. To develop an understanding of what the 
    recurrent network encodes, visualisation techniques alone do not 
    suffice. Therefore, we develop an approach where we formulate and 
    test multiple hypotheses on the information encoded and processed 
    by the network. For each hypothesis, we derive predictions about 
    features of the hidden state representations at each time step, and 
    train 'diagnostic classifiers' to test those predictions. Our 
    results indicate that the networks follow a strategy similar to our 
    hypothesised 'cumulative strategy', which explains the high accuracy 
    of the network on novel expressions, the generalisation to longer 
    expressions than seen in training, and the mild deterioration with 
    increasing length. This is turn shows that diagnostic classifiers 
    can be a useful technique for opening up the black box of neural 
    networks. We argue that diagnostic classification, unlike most 
    visualisation techniques, does scale up from small networks in a 
    toy domain, to larger and deeper recurrent networks dealing with 
    real-life data, and may therefore contribute to a better understanding 
    of the internal dynamics of current state-of-the-art models in 
    natural language processing.
