---
layout: research
title: Our research
updated: 2025-05-08
published: 2025-05-08
---

Most of the research in the lab in focused on Interpretability methods in Artificial Intelligence. We apply them to LLMs, neural speech models, vision-language models, and time series models. We develop new interpretability techniques. Much of our work has been inspired by findings in linguistics, cognitive science and neuroscience, and a long term goal is to 'give back' to those fields, for instance by using AI models as scaffolds to build predictive models of neuroimaging data or by providing 'existence proofs' of possible ways in which networks of neurons might implement specific cognitive functions.

<div class="references">
  <h4>References</h4>
  {% bibliography --cited %}
</div>
